{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbdxDzMP5_kj"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Intrinsic Innovation LLC.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q2ZfbEO6G2Z"
      },
      "source": [
        "A GMRF is an 8-connected, grid-arranged Markov random field with hidden variables, originally proposed in the [Query Training](https://ojs.aaai.org/index.php/AAAI/article/view/17004) AAAI 2021 paper. \n",
        "\n",
        "In this notebook we demonstrate inference and gradient-based learning of a GMRF using PGMax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccA2OnvEqDar"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from jax.example_libraries import optimizers\n",
        "from tqdm.notebook import tqdm\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "############\n",
        "# Load PGMax\n",
        "from pgmax import fgraph, fgroup, infer, vgroup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaCxb3yrT9s8"
      },
      "source": [
        "# Create the noisy MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7UdIg-NT7xe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage.morphology import binary_dilation\n",
        "\n",
        "def contour_mnist(X):\n",
        "  \"\"\"Extract the contours of the MNIST images.\"\"\"\n",
        "  X = X.astype(float) / 255.\n",
        "  # Contour are obtained by dilating the digits\n",
        "  X = (X \u003e 0.5).astype(int)\n",
        "  s = np.zeros((3, 3, 3))\n",
        "  s[1, 1, :3] = 1\n",
        "  s[1, :3, 1] = 1\n",
        "  X += binary_dilation(X, s)\n",
        "  X[X == 1] = -1\n",
        "  X[X == 0] = 1\n",
        "  X[X == -1] = 0\n",
        "  contour_X = np.ones((X.shape[0], 30, 30), int)\n",
        "  contour_X[:, 1:-1, 1:-1] = X\n",
        "  return contour_X\n",
        "  \n",
        "def add_noise_and_remove_contours(\n",
        "    images, \n",
        "    n_spurious_per_image, \n",
        "    p_contour_deletion, \n",
        "    n_add_contour_tries=1000, \n",
        "    seed=0\n",
        "):\n",
        "  \"\"\"Add spurious edges and removes contours at random.\"\"\"\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  noise_patterns = np.array([\n",
        "      [[1, 3, 1], [1, 3, 1], [1, 3, 1]],\n",
        "      [[1, 1, 1], [3, 3, 3], [1, 1, 1]],\n",
        "      [[3, 1, 1], [1, 3, 1], [1, 1, 3]],  \n",
        "      [[1, 1, 3], [1, 3, 1], [3, 1, 1]]\n",
        "  ])\n",
        "  assert noise_patterns.shape[1:] == (3, 3) \n",
        "\n",
        "  N, H, W = images.shape\n",
        "  noisy_images = images.copy()\n",
        "\n",
        "  # Add spurious edges\n",
        "  for n in range(N):\n",
        "    n_spurious = 0\n",
        "    for t in range(n_add_contour_tries):\n",
        "      r, c = np.random.randint(H - 4), np.random.randint(W - 4)\n",
        "      if (noisy_images[n, r: r + 5, c: c + 5] == 1).all():  # all out\n",
        "        idx = np.random.randint(len(noise_patterns))\n",
        "        noisy_images[n, r + 1:r + 4, c + 1:c + 4] = noise_patterns[idx]\n",
        "        n_spurious += 1\n",
        "      if n_spurious == n_spurious_per_image:\n",
        "        break\n",
        "    \n",
        "  # Remove contours\n",
        "  n, r, c = (noisy_images == 0).nonzero()\n",
        "  mask = np.random.binomial(1, p=p_contour_deletion, size=len(r)).astype(bool)\n",
        "  n, r, c = n[mask], r[mask], c[mask]\n",
        "  noisy_images[n, r, c] = 1\n",
        "\n",
        "  # Probability of observing 1 (contour) given label is 0 (border), 1 (out) 2(in)\n",
        "  p_contour = np.array([\n",
        "      1 - mask.mean(),\n",
        "      ((noisy_images == 3).sum() + 0.0) / ((noisy_images == 1).sum() + (noisy_images == 3).sum()),\n",
        "      1e-10  \n",
        "  ])  \n",
        "  noisy_images[noisy_images == 3] = 0\n",
        "  noisy_images[noisy_images \u003e 0] = 1\n",
        "  return noisy_images, p_contour\n",
        "\n",
        "\n",
        "def get_noisy_mnist(dataset=\"test\", n_samples=100):\n",
        "  data = tfds.as_numpy(tfds.load(\"mnist\", split=dataset, batch_size=-1))\n",
        "  if n_samples is None:\n",
        "    X = data[\"image\"][:, :, :, 0]\n",
        "  else:\n",
        "    X = data[\"image\"][:n_samples, :, :, 0]\n",
        "  target_images = contour_mnist(X)\n",
        "  noisy_images, p_contour = add_noise_and_remove_contours(\n",
        "      target_images, \n",
        "      n_spurious_per_image=8, \n",
        "      p_contour_deletion=0.2\n",
        "  )\n",
        "  print(f\"Noisy {dataset} MNIST generated for {n_samples} samples\")\n",
        "  return target_images, noisy_images, p_contour"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc6JJj4M6MEe"
      },
      "source": [
        "# Visualize a trained GMRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPT4Vdru6Ili"
      },
      "outputs": [],
      "source": [
        "target_images, noisy_images, p_contour = get_noisy_mnist(dataset=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFhZ5Ma69aPj"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "folder_name = \"example_data/\"\n",
        "grmf_log_potentials = np.load(open(folder_name + \"gmrf_log_potentials.npz\", 'rb'), allow_pickle=True)\n",
        "\n",
        "n_clones = grmf_log_potentials[\"n_clones\"]\n",
        "p_contour = jax.device_put(np.repeat(p_contour, n_clones))\n",
        "prototype_targets = jax.device_put(\n",
        "    np.array(\n",
        "        [\n",
        "            np.repeat(np.array([1, 0, 0]), n_clones),\n",
        "            np.repeat(np.array([0, 1, 0]), n_clones),\n",
        "            np.repeat(np.array([0, 0, 1]), n_clones),\n",
        "        ]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01jmMZxp9rAI"
      },
      "outputs": [],
      "source": [
        "M, N = target_images.shape[-2:]\n",
        "num_states = np.sum(n_clones)\n",
        "variables = vgroup.NDVarArray(num_states=num_states, shape=(M, N))\n",
        "fg = fgraph.FactorGraph(variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHoMRxlW95gZ"
      },
      "outputs": [],
      "source": [
        "# Create top-down factors\n",
        "top_down = fgroup.PairwiseFactorGroup(\n",
        "    variables_for_factors=[\n",
        "        [variables[ii, jj], variables[ii + 1, jj]]\n",
        "        for ii in range(M - 1)\n",
        "        for jj in range(N)\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Create left-right factors\n",
        "left_right = fgroup.PairwiseFactorGroup(\n",
        "    variables_for_factors=[\n",
        "        [variables[ii, jj], variables[ii, jj + 1]]\n",
        "        for ii in range(M)\n",
        "        for jj in range(N - 1)\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Create diagonal factors\n",
        "diagonal0 = fgroup.PairwiseFactorGroup(\n",
        "    variables_for_factors=[\n",
        "        [variables[ii, jj], variables[ii + 1, jj + 1]]\n",
        "        for ii in range(M - 1)\n",
        "        for jj in range(N - 1)\n",
        "    ],\n",
        ")\n",
        "diagonal1 = fgroup.PairwiseFactorGroup(\n",
        "    variables_for_factors=[\n",
        "        [variables[ii, jj], variables[ii - 1, jj + 1]]\n",
        "        for ii in range(1, M)\n",
        "        for jj in range(N - 1)\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Add factors to the factor graph\n",
        "fg.add_factors([top_down, left_right, diagonal0, diagonal1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1A0xruQ97IR"
      },
      "outputs": [],
      "source": [
        "bp = infer.BP(fg.bp_state, temperature=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll5cfWv696w-"
      },
      "outputs": [],
      "source": [
        "log_potentials = {\n",
        "    top_down: grmf_log_potentials[\"top_down\"],\n",
        "    left_right: grmf_log_potentials[\"left_right\"],\n",
        "    diagonal0: grmf_log_potentials[\"diagonal0\"],\n",
        "    diagonal1: grmf_log_potentials[\"diagonal1\"],\n",
        "}\n",
        "\n",
        "n_plots = 5\n",
        "indices = np.random.permutation(noisy_images.shape[0])[:n_plots]\n",
        "fig, ax = plt.subplots(n_plots, 3, figsize=(12, 4 * n_plots))\n",
        "for plot_idx, idx in tqdm(enumerate(indices), total=n_plots):\n",
        "  noisy_image = noisy_images[idx]\n",
        "  target_image = target_images[idx]\n",
        "  evidence = jnp.log(jnp.where(noisy_image[..., None] == 0, p_contour, 1 - p_contour))\n",
        "  target = prototype_targets[target_image]\n",
        "  marginals = infer.get_marginals(\n",
        "      bp.get_beliefs(\n",
        "          bp.run_bp(\n",
        "              bp.init(\n",
        "                  evidence_updates={variables: evidence},\n",
        "                  log_potentials_updates=log_potentials,\n",
        "              ),\n",
        "              num_iters=15,\n",
        "              damping=0.0,\n",
        "          )\n",
        "      )\n",
        "  )[variables]\n",
        "\n",
        "  pred_image = np.argmax(\n",
        "      np.stack(\n",
        "          [\n",
        "              np.sum(marginals[..., :-2], axis=-1),\n",
        "              marginals[..., -2],\n",
        "              marginals[..., -1],\n",
        "          ],\n",
        "          axis=-1,\n",
        "      ),\n",
        "      axis=-1,\n",
        "  )\n",
        "  ax[plot_idx, 0].imshow(noisy_image)\n",
        "  ax[plot_idx, 0].axis(\"off\")\n",
        "  ax[plot_idx, 1].imshow(target_image)\n",
        "  ax[plot_idx, 1].axis(\"off\")\n",
        "  ax[plot_idx, 2].imshow(pred_image)\n",
        "  ax[plot_idx, 2].axis(\"off\")\n",
        "  if plot_idx == 0:\n",
        "    ax[plot_idx, 0].set_title(\"Input noisy image\", fontsize=30)\n",
        "    ax[plot_idx, 1].set_title(\"Ground truth\", fontsize=30)\n",
        "    ax[plot_idx, 2].set_title(\"GMRF prediction\", fontsize=30)\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXpcAG2q-BHn"
      },
      "source": [
        "# Train the model from scratch\n",
        "\n",
        "The following training loop requires a GPU with at least 11 GB of memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYx6IoP3-AF2"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def loss(noisy_image, target_image, log_potentials):\n",
        "  \"\"\"Computes the cross entropy loss between the predicted marginals and the ground truth.\"\"\"\n",
        "  evidence = jnp.log(jnp.where(noisy_image[..., None] == 0, p_contour, 1 - p_contour))\n",
        "  target = prototype_targets[target_image]\n",
        "  marginals = infer.get_marginals(\n",
        "      bp.get_beliefs(\n",
        "          bp.run_bp(\n",
        "              bp.init(\n",
        "                  evidence_updates={variables: evidence},\n",
        "                  log_potentials_updates=log_potentials,\n",
        "              ),\n",
        "              num_iters=15,\n",
        "              damping=0.0,\n",
        "          )\n",
        "      )\n",
        "  )\n",
        "  logp = jnp.mean(jnp.log(jnp.sum(target * marginals[variables], axis=-1)))\n",
        "  return -logp\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def batch_loss(noisy_images, target_images, log_potentials):\n",
        "  \"\"\"Averages the loss across multiple images.\"\"\"\n",
        "  return jnp.mean(\n",
        "      jax.vmap(loss, in_axes=(0, 0, None), out_axes=0)(\n",
        "          noisy_images, target_images, log_potentials\n",
        "      )\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDFJ0iph99bQ"
      },
      "outputs": [],
      "source": [
        "value_and_grad = jax.jit(jax.value_and_grad(batch_loss, argnums=2))\n",
        "init_fun, opt_update, get_params = optimizers.adam(2e-3)\n",
        "\n",
        "@jax.jit\n",
        "def update(step, batch_noisy_images, batch_target_images, opt_state):\n",
        "  \"\"\"Update the parameters.\"\"\"\n",
        "  value, grad = value_and_grad(\n",
        "      batch_noisy_images, batch_target_images, get_params(opt_state)\n",
        "  )\n",
        "  opt_state = opt_update(step, grad, opt_state)\n",
        "  return value, opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvR9T96j-EFU"
      },
      "outputs": [],
      "source": [
        "# Get the training data\n",
        "target_images_train, noisy_images_train, _ = get_noisy_mnist(dataset=\"train\", n_samples=None)\n",
        "\n",
        "# Initialize the optimizer\n",
        "opt_state = init_fun(\n",
        "    {\n",
        "        top_down: np.random.randn(num_states, num_states),\n",
        "        left_right: np.random.randn(num_states, num_states),\n",
        "        diagonal0: np.random.randn(num_states, num_states),\n",
        "        diagonal1: np.random.randn(num_states, num_states),\n",
        "    }\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "batch_size = 10\n",
        "n_epochs = 10\n",
        "n_batches = noisy_images_train.shape[0] // batch_size\n",
        "with tqdm(total=n_epochs * n_batches) as pbar:\n",
        "  for epoch in range(n_epochs):\n",
        "    indices = np.random.permutation(noisy_images_train.shape[0])\n",
        "    for idx in range(n_batches):\n",
        "      batch_indices = indices[idx * batch_size : (idx + 1) * batch_size]\n",
        "      batch_noisy_images, batch_target_images = (\n",
        "          noisy_images_train[batch_indices],\n",
        "          target_images_train[batch_indices],\n",
        "      )\n",
        "      step = epoch * n_batches + idx\n",
        "      value, opt_state = update(\n",
        "          step, batch_noisy_images, batch_target_images, opt_state\n",
        "      )\n",
        "      pbar.update()\n",
        "      pbar.set_postfix(loss=value)\n",
        "\n",
        "  \n",
        "# Get the trained parameters\n",
        "params = get_params(opt_state)\n",
        "for factor in [top_down, left_right, diagonal0, diagonal1]:\n",
        "  assert params[factor].shape == (num_states, num_states)"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}
